{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KARTIKSINGH542/project1/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQwoGjwNjLFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv('train (1).csv')\n",
        "df = df.dropna(subset=['text'])\n",
        "X = df['text']"
      ],
      "metadata": {
        "id": "sSnNy62SkHv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords', force=True)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    words = tokenizer.tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words and len(word) > 1]\n",
        "    stemmed = [stemmer.stem(word) for word in words]\n",
        "    doc = nlp(\" \".join(stemmed))\n",
        "    lemmatized = [token.lemma_ for token in doc]\n",
        "\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "df['normalized_text'] = df['text'].astype(str).apply(preprocess_tweet)"
      ],
      "metadata": {
        "id": "lfjQvJ_jmGtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "all_text = ' '.join(df['normalized_text'])\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Most Frequent Words in Tweets\", fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rzxfSHP7mWiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_features=5000)\n",
        "X_bow = bow_vectorizer.fit_transform(df['normalized_text'])\n",
        "\n",
        "print(X_bow.shape)"
      ],
      "metadata": {
        "id": "0M1BS0qFtLqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Jrtgw14lwrtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_state = outputs.last_hidden_state  # shape: (1, seq_len, hidden_size)\n",
        "    mean_embedding = last_hidden_state.mean(dim=1).squeeze().numpy()  # shape: (768,)\n",
        "\n",
        "    return mean_embedding"
      ],
      "metadata": {
        "id": "ezNBlthew0BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "bert_embeddings = []\n",
        "for tweet in tqdm(df['normalized_text']):\n",
        "    try:\n",
        "        emb = get_bert_embedding(tweet)\n",
        "        bert_embeddings.append(emb)\n",
        "    except Exception as e:\n",
        "        bert_embeddings.append(np.zeros(768))\n",
        "\n",
        "X_bert = np.array(bert_embeddings)\n",
        "print(X_bert.shape)"
      ],
      "metadata": {
        "id": "uGDBQINTxCai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = X_bert\n",
        "y = df['target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5kqShC9SxOiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "def evaluate_model(name, model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{name} Performance:\")\n",
        "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall   :\", recall_score(y_test, y_pred))\n",
        "    print(\"F1-score :\", f1_score(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "evaluate_model(\"Logistic Regression\", lr_model, X_test, y_test)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "evaluate_model(\"SVM\", svm_model, X_test, y_test)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "evaluate_model(\"Naive Bayes\", nb_model, X_test, y_test)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_conf_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "plot_conf_matrix(y_test, lr_model.predict(X_test), \"Logistic Regression Confusion Matrix\")\n"
      ],
      "metadata": {
        "id": "XEn5UmpXxPU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_roc(models, X_test, y_test):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for name, model in models.items():\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_score = model.predict_proba(X_test)[:, 1]\n",
        "        else:  # For SVM without probability\n",
        "            y_score = model.decision_function(X_test)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_precision_recall(models, X_test, y_test):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for name, model in models.items():\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_score = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_score = model.decision_function(X_test)\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
        "        plt.plot(recall, precision, label=name)\n",
        "\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curves\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_conf_matrix(model, name, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "def show_misclassified(model, name, X_test, y_test, raw_texts):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mis_idx = np.where(y_pred != y_test)[0]\n",
        "\n",
        "    print(f\"\\nüîç Misclassified by {name}:\")\n",
        "    for i in mis_idx[:5]:  # Show top 5\n",
        "        print(f\"\\nTweet: {raw_texts[i]}\")\n",
        "        print(f\"True Label: {y_test[i]}, Predicted: {y_pred[i]}\")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "lr_params = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), lr_params, cv=3, scoring='f1', n_jobs=-1)\n",
        "grid_lr.fit(X_train, y_train)\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "random_svm = RandomizedSearchCV(SVC(probability=True), svm_params, n_iter=5, cv=3, scoring='f1', n_jobs=-1, random_state=42)\n",
        "random_svm.fit(X_train, y_train)\n",
        "best_svm = random_svm.best_estimator_\n",
        "\n",
        "nb_model = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": best_lr,\n",
        "    \"SVM\": best_svm,\n",
        "    \"Naive Bayes\": nb_model\n",
        "}\n",
        "\n",
        "plot_roc(models, X_test, y_test)\n",
        "plot_precision_recall(models, X_test, y_test)\n",
        "\n",
        "for name, model in models.items():\n",
        "    plot_conf_matrix(model, name, X_test, y_test)\n",
        "\n",
        "raw_texts = df['text'].values  # or df['normalized_text'].values\n",
        "for name, model in models.items():\n",
        "    show_misclassified(model, name, X_test, y_test, raw_texts)\n"
      ],
      "metadata": {
        "id": "gT1tPlawx48Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}